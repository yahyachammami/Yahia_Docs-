{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb04fa67",
      "metadata": {
        "id": "bb04fa67"
      },
      "source": [
        "  \n",
        "  \n",
        "#            Chapter 2: NLP tools and libraries\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/7065401/55025843-7d99a280-4fe0-11e9-938a-4879d95c4130.png\"\n",
        "    style=\"width:150px; float: right; margin: 0 40px 40px 40px;\"></img>\n",
        "    \n",
        "<img src=\"https://www.searchenginejournal.com/wp-content/uploads/2020/08/an-introduction-to-natural-language-processing-with-python-for-seos-5f3519eeb8368-1520x800.webp\" style=\"width:300px; float: left; margin: 0 40px 40px 40px;\"></img>\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a353712",
      "metadata": {
        "id": "2a353712"
      },
      "source": [
        "# 1- Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c236290",
      "metadata": {
        "id": "3c236290"
      },
      "source": [
        "## a- Context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37d84eb0",
      "metadata": {
        "id": "37d84eb0"
      },
      "source": [
        "![image.png ](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "346fa9a5",
      "metadata": {
        "id": "346fa9a5"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3993913c",
      "metadata": {
        "id": "3993913c"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d4df7e8",
      "metadata": {
        "id": "4d4df7e8"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5edad557",
      "metadata": {
        "id": "5edad557"
      },
      "source": [
        "Knowledge is important to correctly understand what is being said\n",
        "### ==> Semantics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac0ee36",
      "metadata": {
        "id": "eac0ee36"
      },
      "source": [
        "## b- Specializations within NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f937a5aa",
      "metadata": {
        "id": "f937a5aa"
      },
      "source": [
        "**NLP** serves as the overarching umbrella term that encompasses various subfields, types, or branches of natural language processing and understanding. NLP includes tasks related to text analysis, language generation, and language understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb99e097",
      "metadata": {
        "id": "fb99e097"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffb2f86c",
      "metadata": {
        "id": "ffb2f86c"
      },
      "source": [
        "**NLU (Natural Language Understanding)** is a specialized area of NLP that emphasizes the ability to understand and interpret the meaning, intent, and context of human language. NLU focuses on tasks such as sentiment analysis, chatbot interactions, and named entity recognition.\n",
        "\n",
        "**NLG (Natural Language Generation)** is another specialized area of NLP that focuses on the generation of human-like text or spoken language by computers. NLG involves tasks such as text summarization, content generation, and automated report writing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03be5122",
      "metadata": {
        "id": "03be5122"
      },
      "source": [
        "**Both? Yes, it's possible:**\n",
        "\n",
        "Examples:\n",
        "- Chatbots and Virtual Assistants:\n",
        "Chatbots need to understand user queries or statements (NLU) and generate coherent and contextually relevant responses (NLG).\n",
        "- Conversational Recommender Systems:\n",
        "Systems that engage users in a dialogue to recommend products, services, or content must understand user preferences (NLU) and present personalized recommendations (NLG).\n",
        "- Content Summarization and Paraphrasing:\n",
        "Systems that summarize long articles or rephrase sentences must grasp the content (NLU) and generate concise summaries or alternative phrasings (NLG)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5cbe508",
      "metadata": {
        "id": "a5cbe508"
      },
      "source": [
        "**Is chat GPT NLP or NLG?**\n",
        "\n",
        "He said : I am primarily an NLU (Natural Language Understanding) model. My main function is to understand and generate human-like text based on the input and queries I receive. While I can perform some NLG (Natural Language Generation) tasks, my primary strength lies in NLU, which enables me to comprehend and provide informative responses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "094b35ea",
      "metadata": {
        "id": "094b35ea"
      },
      "source": [
        "## 2- End-to-end NLP pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393e6fdb",
      "metadata": {
        "id": "393e6fdb"
      },
      "source": [
        "A Natural Language Processing (NLP) pipeline is a series of data processing steps or components used to convert *raw text data* into a structured format that can be analyzed by NLP models and algorithms. These pipelines are designed to perform various tasks related to understanding and extracting information from text data. Here are the typical components of an NLP pipeline:\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "1. **Text Preprocessing**:\n",
        "\n",
        "*a. Standard steps:*\n",
        "   - Tokenization: Splitting text into individual words or tokens.\n",
        "   - Lowercasing: Converting all text to lowercase for consistency.\n",
        "   - Stopword Removal: Removing common words (e.g., \"and,\" \"the\") that do not carry significant meaning.\n",
        "   - Punctuation Removal: Removing punctuation marks.\n",
        "   - Spell Correction: Correcting common spelling errors.\n",
        "\n",
        "*b. Text Cleaning:*\n",
        "   - Removing HTML tags, special characters, or unwanted symbols.\n",
        "   - Handling contractions (e.g., \"I'm\" to \"I am\").\n",
        "   - Removing or replacing URLs, email addresses, and other sensitive information.\n",
        "\n",
        "*c. Text Normalization:*\n",
        "   - Stemming: Reducing words to their root form (e.g., \"running\" to \"run\").\n",
        "   - Lemmatization: Reducing words to their base or dictionary form (e.g., \"better\" to \"good\").\n",
        "\n",
        "2. **Feature Extraction**:\n",
        "   - Bag of Words (BoW): Creating a matrix of word frequencies or presence/absence.\n",
        "   - Term Frequency-Inverse Document Frequency (TF-IDF): Assigning weights to words based on their importance in a document.\n",
        "   - Word Embeddings: Representing words as dense vector representations (e.g., Word2Vec, GloVe).\n",
        "   - Named Entity Recognition (NER):  Identifying and classifying entities (e.g., names of people, organizations, locations) in text.\n",
        "   - Part-of-Speech Tagging (POS): Assigning grammatical tags (e.g., noun, verb, adjective) to words in a sentence. These POS tags can be used as features, especially in syntactic or grammatical analysis tasks.\n",
        "\n",
        "2. **Modeling**:  \n",
        "    - building and training machine learning or deep learning models to perform specific natural language processing tasks\n",
        "    - selecting an appropriate model architecture\n",
        "    - training the model on labeled data\n",
        "    - tuning hyperparameters\n",
        "    - evaluating its performance using suitable metrics.\n",
        "    - The goal is to create a model that can accurately handle the NLP task at hand, such as text classification, sentiment analysis, machine translation, or text generation.\n",
        "    - It's not a linear process\n",
        "\n",
        "\n",
        "\n",
        "The specific components and order of these steps in an NLP pipeline can vary depending on the task and the goals of the analysis. NLP pipelines are commonly used in applications like text classification, information retrieval, chatbots, and more. They play a crucial role in extracting meaningful insights from unstructured text data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fcc0b0a",
      "metadata": {
        "id": "0fcc0b0a"
      },
      "source": [
        "## 3- NLP libraries in Python: SpaCy vs NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1fd5580",
      "metadata": {
        "id": "b1fd5580"
      },
      "source": [
        "### a- What is NLTK?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d30aff0b",
      "metadata": {
        "id": "d30aff0b"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "**NLTK (Natural Language Toolkit)**: NLTK is a comprehensive library for NLP that provides easy-to-use interfaces for over 50 corpora and lexical resources, such as WordNet. It also includes various text processing libraries.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br><br>\n",
        "<br><br>\n",
        "<br><br>\n",
        "<br><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b43bd9",
      "metadata": {
        "id": "45b43bd9"
      },
      "source": [
        "## b- What is spaCy?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66338bef",
      "metadata": {
        "id": "66338bef"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "2. **spaCy**: spaCy is a fast and efficient NLP library that offers pre-trained models for several languages. It's known for its ease of use and speed in tokenization, part-of-speech tagging, named entity recognition, and more.\n",
        "<br><br>\n",
        "<br><br>\n",
        "<br><br>\n",
        "<br><br>\n",
        "<br><br>\n",
        "<br><br>\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eed594a",
      "metadata": {
        "id": "9eed594a"
      },
      "source": [
        "### c- SpaCy vS NLTK: who wins th battle ? 😉"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4a318d9",
      "metadata": {
        "id": "d4a318d9"
      },
      "source": [
        "Spacy is an Object-Oriented library. It's designed around the concept of \"processing pipelines\" where you create a \"nlp\" object that contains various NLP components such as tokenizers, part-of-speech taggers, and named entity recognizers.\n",
        "NLTK, on the other hand, is a more modular and procedural library, string processing library.\n",
        "\n",
        "Here is an example to show the difference between them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d511c7de",
      "metadata": {
        "id": "d511c7de"
      },
      "source": [
        "### Example:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3565be31",
      "metadata": {
        "id": "3565be31"
      },
      "source": [
        "#### i- Package Installation: SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Package Installation: SpaCy\n",
        "\n",
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PnXcrNNduSN",
        "outputId": "cc78c7d7-82f8-4af6-b663-ef233f54ab19"
      },
      "id": "6PnXcrNNduSN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.1)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d74db4d",
      "metadata": {
        "id": "2d74db4d"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f0bba23",
      "metadata": {
        "scrolled": true,
        "id": "4f0bba23"
      },
      "outputs": [],
      "source": [
        "#checking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554f8402",
      "metadata": {
        "id": "554f8402"
      },
      "outputs": [],
      "source": [
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ea34a0",
      "metadata": {
        "id": "90ea34a0"
      },
      "source": [
        "#### ii- Tokenization with SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Tokenization with SpaCy\n",
        "text = \"Nlp is very enjoyable.It obviously has a fun part too. think of what makes NLP cool ?\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7THIWYheRWg",
        "outputId": "d35f415a-f0af-4754-97ba-72fb57c0b7a2"
      },
      "id": "g7THIWYheRWg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nlp is very enjoyable.\n",
            "It obviously has a fun part too.\n",
            "think of what makes NLP cool ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee93fa2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee93fa2d",
        "outputId": "2d5a4a4f-64a9-4271-916a-9a28f1af7a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mrs. Tasnime likes to tokenize texts!\n",
            "she is having a lot of fun.\n"
          ]
        }
      ],
      "source": [
        "# let's confuse spacy. add Mrs. and see if he will consider it as a sentence\n",
        "text =\" Mrs. Tasnime likes to tokenize texts! she is having a lot of fun.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d215c6cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d215c6cf",
        "outputId": "f3d8031f-9b93-4eed-973c-09ad297b5f17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "Mrs.\n",
            "Tasnime\n",
            "likes\n",
            "to\n",
            "tokenize\n",
            "texts\n",
            "!\n",
            "she\n",
            "is\n",
            "having\n",
            "a\n",
            "lot\n",
            "of\n",
            "fun\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "# let's confuse spacy. add Mrs. and see if he will consider it as a sentence\n",
        "text =\" Mrs. Tasnime likes to tokenize texts! she is having a lot of fun.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c693bf74",
      "metadata": {
        "id": "c693bf74"
      },
      "source": [
        "#### iii- NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc577f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fc577f6",
        "outputId": "27f1d512-995a-4d6c-8b71-272039892fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "# The Punkt tokenizer is a pre-trained tokenizer provided by NLTK for various languages\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e85e6de",
      "metadata": {
        "id": "3e85e6de"
      },
      "source": [
        "#### iv-  Tokenization with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce045c68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce045c68",
        "outputId": "6f23ed52-0f39-4969-e501-29fe73f40d95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Nlp is very enjoyable.',\n",
              " 'It obviously has a fun part too.',\n",
              " 'think of what makes NLP cool ?']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# TAB: many different tokenizers are presented\n",
        "# provides customization techniques\n",
        "# allow you to select specific settings\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(\"Nlp is very enjoyable. It obviously has a fun part too. think of what makes NLP cool ?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5116d418",
      "metadata": {
        "id": "5116d418"
      },
      "source": [
        "You put strings as an input, you get strings as an output ==> NLTK: String processing library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f05f4198",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f05f4198",
        "outputId": "92618614-f783-40fb-a64f-4b9798fb30e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Nlp',\n",
              " 'is',\n",
              " 'very',\n",
              " 'enjoyable',\n",
              " '.',\n",
              " 'It',\n",
              " 'obviously',\n",
              " 'has',\n",
              " 'a',\n",
              " 'fun',\n",
              " 'part',\n",
              " 'too',\n",
              " '.',\n",
              " 'think',\n",
              " 'of',\n",
              " 'what',\n",
              " 'makes',\n",
              " 'NLP',\n",
              " 'cool',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(\"Nlp is very enjoyable. It obviously has a fun part too. think of what makes NLP cool ?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d6e4a9",
      "metadata": {
        "id": "d8d6e4a9"
      },
      "source": [
        "#### Conclusion :\n",
        "**SpaCy** is often preferred for production-level NLP applications due to its speed, efficiency, and ease of use. **NLTK** is more versatile and educational, making it a good choice for research and experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e829c9a3",
      "metadata": {
        "id": "e829c9a3"
      },
      "source": [
        "## 4- Other libraries for NLP in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7949e16",
      "metadata": {
        "id": "f7949e16"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "1. **Gensim**: Gensim is a library for topic modeling and document similarity analysis. It's particularly useful for training word embeddings using Word2Vec and Doc2Vec models.\n",
        "\n",
        "\n",
        "💡 *Word embeddings* are vector representations of words in a high-dimensional space, typically in the form of real-valued vectors.\n",
        "\n",
        "💡Gensim is considered super-fast because it is optimized for efficient memory usage and performance. It utilizes techniques like streaming and incremental processing, enabling it to handle large datasets and train models quickly.\n",
        "\n",
        "💡 How it works: In the context of data cleaning, streaming refers to processing data as it becomes available, rather than loading the entire dataset into memory at once. It involves sequentially reading and processing data in smaller chunks,\n",
        "<br><br><br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c330684b",
      "metadata": {
        "id": "c330684b"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "2. **scikit-learn**: While scikit-learn is primarily used for machine learning, it includes modules like CountVectorizer, TfidfVectorizer, TfidfTransformer, LogisticRegression, MultinomialNB, etc. for text feature extraction and text classification, making it a valuable tool for NLP tasks.\n",
        "\n",
        "<br><br><br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9423d936",
      "metadata": {
        "id": "9423d936"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "3. **Transformers (Hugging Face)**: Transformers is a library by Hugging Face that provides pre-trained models for a wide range of NLP tasks, including BERT, GPT-2, and more. It's widely used for tasks like text classification, translation, and text generation.\n",
        "\n",
        "💡 Hugging Face is a company and open-source community that has made significant contributions to the field of Natural Language Processing (NLP).\n",
        "\n",
        "💡 BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model for natural language understanding and representation. It was introduced by Google AI researchers in a 2018 paper titled \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\"\n",
        "\n",
        "\n",
        "\n",
        "<br><br><br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab7ce8b",
      "metadata": {
        "id": "eab7ce8b"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "4. **TextBlob**: TextBlob is a simple NLP library that provides a high-level API for diving into common NLP tasks.\n",
        "\n",
        "\n",
        "💡 TextBlob is an easy-to-use library for natural language processing (NLP) that offers a user-friendly way to perform common NLP tasks without needing in-depth knowledge of NLP techniques.\n",
        "\n",
        "💡 API (Application Programming Interface) a set of rules and protocols that allows different software applications to communicate with each other.\n",
        "\n",
        "<br><br><br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c41227",
      "metadata": {
        "id": "38c41227"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "5. **fastText**: fastText, developed by Facebook, is an open-source, free, lightweight library that allows users to learn text representations and perform text classification tasks.\n",
        "\n",
        "<br><br><br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e60d11",
      "metadata": {
        "id": "86e60d11"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "6 and 7. **PyTorch and TensorFlow**: While not NLP libraries per se, these deep learning frameworks are commonly used for building and training neural network models for various NLP tasks. They offer flexibility for custom model development.\n",
        "<br><br><br><br><br><br><br>\n",
        "\n",
        "\n",
        "💡 PyTorch is a popular ML library for Python based on Torch, which is an ML library implemented in C. It was originally developed by **Facebook**, but is now used by Twitter, Salesforce, and many other major organizations and businesses.\n",
        "\n",
        "\n",
        "💡 Tensorflow: Originally developed by **Google**, TensorFlow is an open-source library for high-performance numerical computation using data flow graphs.\n",
        "Under the hood, it’s actually a framework for creating and running computations involving tensors. The principal application for TensorFlow is in neural networks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c1567f6",
      "metadata": {
        "id": "2c1567f6"
      },
      "source": [
        "These libraries cover a wide range of NLP tasks, from basic text processing to advanced machine learning and deep learning tasks. The choice of library often depends on the specific NLP task you're working on and your preference for ease of use, performance, and available pre-trained models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}